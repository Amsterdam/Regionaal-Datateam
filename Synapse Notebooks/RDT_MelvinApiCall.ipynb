{
  "description": null,
  "sessionProperties": {
    "driverMemory": "28g",
    "driverCores": 4,
    "executorMemory": "28g",
    "executorCores": 4,
    "numExecutors": 2
  },
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": null
    },
    "a365ComputeOptions": {
      "id": "/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-redacted/providers/Microsoft.Synapse/workspaces/redacted/bigDataPools/redacted",
      "name": "redacted",
      "type": "Spark",
      "endpoint": "https://redacted.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/redacted",
      "auth": {
        "type": "AAD",
        "authResource": "https://dev.azuresynapse.net",
        "authHeader": null
      },
      "sparkVersion": "3.1",
      "nodeCount": 10,
      "cores": 4,
      "memory": 28,
      "extraHeader": null
    },
    "sessionKeepAliveTimeout": 30,
    "saveOutput": true,
    "enableDebugMode": false
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# MelvinAPICall\r\n",
        "Gebruikt NDW credentials om in te loggen op Melvin en data op te halen\r\n",
        "\r\n",
        "Via API-call worden alle restrictions/situations met bijbehorende timeperiods ingelezen in batches van 1 maand, en als 3 tabellen opgeslagen. Ze zijn te koppelen op basis van melvin_id.\r\n",
        "\r\n",
        "Op basis van deze data worden de planningsconflicten berekend"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": null,
      "source": [
        "from requests import get, post\r\n",
        "from json import loads, dumps\r\n",
        "import pandas as pd\r\n",
        "from sqlalchemy import create_engine\r\n",
        "import datetime\r\n",
        "import numpy as np\r\n",
        "from os import environ"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# get secret values from keyvault\r\n",
        "melvin_account_username = TokenLibrary.getSecret(\"redacted\", \"redacted\", \"redacted\")\r\n",
        "melvin_account_password = TokenLibrary.getSecret(\"redacted\", \"redacted\", \"redacted\")\r\n",
        "postgres_pw = TokenLibrary.getSecret(\"redacted\", \"redacted\", \"redacted\")"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create token"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# POST Data containing login information\r\n",
        "data={\r\n",
        "    \"client_id\": \"melvin-frontend-test\",\r\n",
        "    \"username\": melvin_account_username,\r\n",
        "    \"password\": melvin_account_password,\r\n",
        "    \"grant_type\":\"password\"\r\n",
        "}\r\n",
        "# Authenticate yourself by generating a token\r\n",
        "# resp=post('https://keycloak.ndwcloud.nu/auth/realms/ndw/protocol/openid-connect/token', data=data)\r\n",
        "resp=post('https://iam.ndw.nu/auth/realms/ndw/protocol/openid-connect/token', data=data)\r\n",
        "# print(resp.text)\r\n",
        "token=loads(resp.text)[\"access_token\"]"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Create DB connection"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "url_str=f\"postgresql://redacted:{postgres_pw}@redacted/rdt_dev\"\r\n",
        "engine = create_engine(url_str)"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Use the token in the headers in the next request to the company API\r\n",
        "headers = {'accept': 'application/json', 'Authorization': f'Bearer {token}', 'melvin-user-token': f'Bearer {token}'}\r\n",
        "# print(headers)\r\n",
        "data={\r\n",
        "    \"statuses\": [\"START\", \"INITIAL\", \"CONCEPT\", \"FINAL\"],\r\n",
        "    \"activityTypes\": [\"WORK\", \"EVENT\"],\r\n",
        "    \"restrictionTypes\": [\"COMPLETE\"],\r\n",
        "    \"includeDetours\": True,\r\n",
        "    \"areaBuffer\": 10,\r\n",
        "    \"areaIds\": [272],\r\n",
        "    \"startPeriod\": \"2023-01-01T00:00:00Z\",\r\n",
        "    \"endPeriod\": \"2023-01-01T23:59:59Z\"\r\n",
        "}"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Functions to write paged data to postgres"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def write_or_finderror(df,table):\r\n",
        "    try:\r\n",
        "        df.to_sql(table, engine, schema=\"stg_conflicten\", if_exists=mode, index=False, index_label=None)\r\n",
        "    except:\r\n",
        "        print('Error writing datato table: '+table)\r\n",
        "        for col in df.columns:\r\n",
        "            print(col)\r\n",
        "            try:\r\n",
        "                df[col].to_sql(\"ndw_werkzaamheden_raw_errors\", engine, schema=\"stg_conflicten\", if_exists='replace', index=False, index_label=None)\r\n",
        "            except:\r\n",
        "                print(df[col])\r\n",
        "                print(df)\r\n",
        "                df.to_clipboard()\r\n",
        "                raise"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def addToTable(data,mode):\r\n",
        "    # data=resp_json\r\n",
        "    # Flatten data\r\n",
        "    df_nested_list = pd.json_normalize(data, record_path =['features'])\r\n",
        "    # df_nested_list_periods = pd.json_normalize(data, record_path =['features','periods'])\r\n",
        "\r\n",
        "    df_nested_list = df_nested_list.replace([np.inf, -np.inf], np.nan)\r\n",
        "    df_nested_list = df_nested_list.fillna(0)\r\n",
        "\r\n",
        "    # split nested structure, both have same id column\r\n",
        "    df_sit = df_nested_list.loc[df_nested_list['properties.type'] == \"SITUATION\"]\r\n",
        "    df_res = df_nested_list.loc[df_nested_list['properties.type'] == \"RESTRICTION\"]\r\n",
        "    # df_period = df_nested_list['properties.periods'].explode()\r\n",
        "    # df_period = pd.json_normalize(df_period)\r\n",
        "    df_per=df_sit[[\"id\", \"properties.periods\"]]\r\n",
        "    df_per=df_per.rename(columns={\"id\":\"sit_id\"})\r\n",
        "    df_per=df_per.explode('properties.periods')\r\n",
        "    df_per=df_per.dropna()\r\n",
        "    df_per1=df_per['sit_id']\r\n",
        "    df_per1=df_per1.reset_index()\r\n",
        "    df_per2=pd.json_normalize(df_per['properties.periods'])\r\n",
        "    df_per2=df_per2.reset_index()\r\n",
        "    df_period = pd.concat([df_per1,df_per2], axis=1)\r\n",
        "    df_period = df_period.fillna(0)\r\n",
        "    df_sit.drop(columns=[\"properties.periods\",\"properties.attachments\"],inplace=True)\r\n",
        "    df_res.drop(columns=[\"properties.periods\",\"properties.attachments\"],inplace=True)\r\n",
        "    df_res[\"geometry.coordinates\"]=df_res[\"geometry.coordinates\"].apply(str)\r\n",
        "\r\n",
        "    # split [lon, lat] into 2 columns for geojson format\r\n",
        "    df_sit['longitude'], df_sit['latitude'] = zip(*list(df_sit['geometry.coordinates'].values))\r\n",
        "\r\n",
        "    # write\r\n",
        "    write_or_finderror(df_sit,\"ndw_werkzaamheden_situation_raw\")\r\n",
        "    write_or_finderror(df_res,\"ndw_werkzaamheden_restriction_raw\")\r\n",
        "    write_or_finderror(df_period,\"ndw_werkzaamheden_period_raw\")"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Writing paged (by date) data to postgres"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Results are paged\r\n",
        "# total_pages=math.ceil(int(restrictions[\"totalCount\"])/25)\r\n",
        "# print(total_pages)\r\n",
        "# for page in range(1,total_pages):\r\n",
        "\r\n",
        "# page results per month, going back 1 month, and forward 15 months\r\n",
        "base = datetime.datetime.today().replace(day=1,hour=0,minute=0,second=0,microsecond=0)\r\n",
        "base=(base.replace(day=1)-datetime.timedelta(days=3)).replace(day=1)\r\n",
        "date_list=[base]\r\n",
        "for x in range(1,15):\r\n",
        "    date_list.append((date_list[x-1].replace(day=28)+datetime.timedelta(days=4)).replace(day=1))\r\n",
        "# for x in range(1,15):\r\n",
        "#     date_list.append(date_list[x-1]+datetime.timedelta(days=1))\r\n",
        "\r\n",
        "mode='replace'\r\n",
        "for page in range(0,len(date_list)-1):\r\n",
        "    # set 'page'\r\n",
        "    data['startPeriod']=date_list[page].isoformat()+\"Z\"\r\n",
        "    data['endPeriod']=(date_list[page+1]-datetime.timedelta(minutes=1)).isoformat()+\"Z\"\r\n",
        "    print('Page '+str(page+1)+'. Van '+str(data['startPeriod'])+' tot '+str(data['endPeriod'])+'.')\r\n",
        "\r\n",
        "    # Do request\r\n",
        "    resp=post('https://melvin.ndw.nu/melvinservice/rest/export', headers=headers, json=data)\r\n",
        "    # print(resp.text)\r\n",
        "    resp_json=loads(resp.text)\r\n",
        "\r\n",
        "    # add to table\r\n",
        "    addToTable(resp_json,mode)\r\n",
        "    mode='append'"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 20
    }
  ],
  "entityState": null,
  "renameOperationDetails": null,
  "targetSparkConfiguration": null
}