{
  "description": null,
  "sessionProperties": {
    "driverMemory": "28g",
    "driverCores": 4,
    "executorMemory": "28g",
    "executorCores": 4,
    "numExecutors": 2
  },
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": null
    },
    "a365ComputeOptions": {
      "id": "/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-redacted/providers/Microsoft.Synapse/workspaces/redacted/bigDataPools/redacted",
      "name": "redacted",
      "type": "Spark",
      "endpoint": "https://redacted.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/redacted",
      "auth": {
        "type": "AAD",
        "authResource": "https://dev.azuresynapse.net",
        "authHeader": null
      },
      "sparkVersion": "3.1",
      "nodeCount": 10,
      "cores": 4,
      "memory": 28,
      "extraHeader": null
    },
    "sessionKeepAliveTimeout": 30,
    "saveOutput": true,
    "enableDebugMode": false
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "metadata": null,
      "source": [
        "import psycopg2\r\n",
        "import pandas as pd\r\n",
        "from sqlalchemy import create_engine, types\r\n",
        "import math\r\n",
        "from requests import get, post\r\n",
        "from json import loads, dumps"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "postgres_pw = TokenLibrary.getSecret(\"redacted\", \"redacted\", \"redacted\")\r\n",
        "\r\n",
        "url_str=f\"postgresql://redacted:{postgres_pw}@redacted/rdt_dev\"\r\n",
        "engine = create_engine(url_str)"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 48
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Get bearer token from Matrixian platform"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "matrixian_user = TokenLibrary.getSecret(\"redacted\", \"matrixian-account-user\", \"redacted\")\r\n",
        "matrixian_pw = TokenLibrary.getSecret(\"redacted\", \"matrixian-account-pass\", \"redacted\")\r\n",
        "data = {\r\n",
        "            'grant_type': 'password',         # Must always be \"password\"\r\n",
        "            'username': matrixian_user,\r\n",
        "            'password': matrixian_pw             # Your login password\r\n",
        "}\r\n",
        "# Authenticate by generating a token\r\n",
        "token = loads(post('https://api.matrixiangroup.com/token', data=data).text)[\"access_token\"]\r\n",
        "# Use the token in the headers in the requests to the company API\r\n",
        "headers = {'accept': 'application/json', 'Authorization': f'Bearer {token}' }\r\n",
        "\r\n",
        "# first request, to calculate total number of pages/items\r\n",
        "params = ()\r\n",
        "# Send a request to the company API using our token\r\n",
        "resp=get('https://api.matrixiangroup.com/logistic/geo-object/vehicle-restriction', headers=headers, params=params, verify=False)\r\n",
        "restrictions = loads(resp.text)"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "(Functions to) write to postgres"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# simple runs pandas to_sql\r\n",
        "# if it encounters an error it will try to write data per column, indicating which column contains the problematic data\r\n",
        "def write_or_finderror(df,table):\r\n",
        "    try:\r\n",
        "        df.to_sql(table, engine, schema=\"stg_bordenanalyse\", if_exists=mode, index=False, index_label=None,dtype={\"constraint.and\": types.JSON})\r\n",
        "    except:\r\n",
        "        print('Error writing data to table: '+table)\r\n",
        "        for col in df.columns:\r\n",
        "            print(col)\r\n",
        "            try:\r\n",
        "                df[col].to_sql(\"error\", engine, schema=\"stg_bordenanalyse\", if_exists='replace', index=False, index_label=None)\r\n",
        "            except:\r\n",
        "                print(df[col])\r\n",
        "                print(df)\r\n",
        "                df.to_clipboard()\r\n",
        "                raise"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 50
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def addToTable(data,mode):\r\n",
        "    # Flatten data\r\n",
        "    # print(data)\r\n",
        "    df_nested_list = pd.json_normalize([data], record_path =['items'])\r\n",
        "    df_constraint = pd.json_normalize(data,record_path=['items','constraint','and'],meta=[['items','id']])\r\n",
        "    \r\n",
        "    df_coords = pd.json_normalize(data, record_path =['items'],max_level=0)\r\n",
        "    df_coords = pd.concat([df_coords['id'],df_coords['geometry'].apply(str)],axis=1)\r\n",
        "    \r\n",
        "    \r\n",
        "    # Some pages of data have fewer columns\r\n",
        "    cols=['id','cityName','remarks','createdDate','updatedDate','trafficDecision','report.id','report.name','report.type','report.description',\r\n",
        "        'report.files','report.status','report.comments','report.createdDate','report.updatedDate','report.user.id','report.areaOfResponsibility','constraint.type',\r\n",
        "        'administrativeReference.maximumSingleAxleWeight','administrativeReference.lzv','administrativeReference.municipalityName','type.type'];\r\n",
        "    for col in cols:\r\n",
        "        if col not in df_nested_list:\r\n",
        "            df_nested_list[col] = None\r\n",
        "            print(\"Adding col \"+col+\" to df\")\r\n",
        "\r\n",
        "    # Some pages of data have to many columns\r\n",
        "    df_nested_list=df_nested_list[cols]\r\n",
        "\r\n",
        "    # write\r\n",
        "    write_or_finderror(df_nested_list,\"matrixian_vehiclerestrictions\")\r\n",
        "    write_or_finderror(df_constraint,\"matrixian_vehiclerestrictions_constraints\")\r\n",
        "    write_or_finderror(df_coords,\"matrixian_vehiclerestrictions_geom\")"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 76
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# because the matrixian endpoint doesn't support verification, and this generates a ton of warnings, disable this specific warning\r\n",
        "import warnings\r\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\r\n",
        "warnings.simplefilter('ignore',InsecureRequestWarning)"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 52
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The actual API call"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Results are paged, 25 items per page\r\n",
        "total_pages=math.ceil(int(restrictions[\"totalCount\"])/25)\r\n",
        "# print(total_pages)\r\n",
        "mode='replace'\r\n",
        "for page in range(1,total_pages):\r\n",
        "    # Set the GET parameters\r\n",
        "    params = (('page',str(page)),)\r\n",
        "    # Send a request to the company API using our token\r\n",
        "    resp=get('https://api.matrixiangroup.com/logistic/geo-object/vehicle-restriction', headers=headers, params=params, verify=False)\r\n",
        "    restrictions = loads(resp.text)\r\n",
        "    \r\n",
        "    addToTable(restrictions,mode)\r\n",
        "    mode='append'\r\n",
        "    \r\n",
        "    print(\"\\r\",\"Values of page \"+str(page)+\"/\"+str(total_pages-1)+\" inserted\")"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 77
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Updating metadata"
      ],
      "attachments": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import psycopg2\r\n",
        "try:\r\n",
        "    postgres_pw = TokenLibrary.getSecret(\"redacted\", \"redacted\", \"redacted\")\r\n",
        "    conn = psycopg2.connect(dbname=\"rdt_dev\",user=\"redacted\",host=\"redacted\",password=postgres_pw)\r\n",
        "    conn.autocommit = True\r\n",
        "    cur=conn.cursor()\r\n",
        "    print(\"Database connected\")\r\n",
        "except:\r\n",
        "    print(\"I am unable to connect to the database\")"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 78
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "sql_meta='''\r\n",
        "CALL update_metadata('rdt_dev'::VARCHAR,'stg_bordenanalyse'::VARCHAR,'matrixian_vehiclerestrictions'::VARCHAR,NOW(),'RDT_Bordenanalyse_MatrixianApiCall'::VARCHAR,'data ingest'::VARCHAR);\r\n",
        "CALL update_metadata('rdt_dev'::VARCHAR,'stg_bordenanalyse'::VARCHAR,'matrixian_vehiclerestrictions_constraints'::VARCHAR,NOW(),'RDT_Bordenanalyse_MatrixianApiCall'::VARCHAR,'data ingest'::VARCHAR);\r\n",
        "CALL update_metadata('rdt_dev'::VARCHAR,'stg_bordenanalyse'::VARCHAR,'matrixian_vehiclerestrictions_geom'::VARCHAR,NOW(),'RDT_Bordenanalyse_MatrixianApiCall'::VARCHAR,'data ingest'::VARCHAR);'''\r\n",
        "cur.execute(sql_meta)"
      ],
      "attachments": null,
      "outputs": [],
      "execution_count": 79
    }
  ],
  "entityState": null,
  "renameOperationDetails": null,
  "targetSparkConfiguration": null
}